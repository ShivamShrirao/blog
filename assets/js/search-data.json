{
  
    
        "post0": {
            "title": "Implementing Transformer in Pytorch",
            "content": "!spacy download de_core_news_sm -q !spacy download en_core_web_sm -q . ✔ Download and installation successful You can now load the model via spacy.load(&#39;de_core_news_sm&#39;) ✔ Download and installation successful You can now load the model via spacy.load(&#39;en_core_web_sm&#39;) . %pip install -q wandb . References . https://www.tensorflow.org/text/tutorials/transformer | https://pytorch.org/tutorials/beginner/translation_tansformer.html | https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html | https://arxiv.org/abs/1706.03762 | . import torch import torch.nn as nn import torch.nn.functional as F . import numpy as np import matplotlib.pyplot as plt from tqdm.notebook import tqdm from collections import OrderedDict . plt.rcParams[&quot;figure.facecolor&quot;] = &quot;white&quot; . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; . Position Encoding . $ Large PE_{(pos,2i)} = sin left( frac{pos}{10000^{ frac{2i}{d_{model}}}} right) $ . $ Large PE_{(pos,2i+1)} = cos left( frac{pos}{10000^{ frac{2i}{d_{model}}}} right) $ . def positional_encoding(max_seq_len, d_model): pos = np.arange(max_seq_len)[:, None] # [max_seq_len, 1] i = np.arange(d_model)[None,:] # [1, d_model] angles = (pos / (10_000**(2*i/d_model))).astype(np.float32) angles[:,0::2] = np.sin(angles[:,0::2]) angles[:,1::2] = np.cos(angles[:,1::2]) return angles[None,:] # [1, max_seq_len, d_model] . max_seq_len, d_model = 2048, 512 pe = positional_encoding(max_seq_len, d_model)[0] pe = pe.reshape(max_seq_len, d_model//2, 2) pe = pe.transpose(2,1,0) pe = pe.reshape(d_model, max_seq_len) plt.pcolormesh(pe, cmap=&#39;RdBu&#39;) plt.ylabel(&#39;Depth&#39;) plt.xlabel(&#39;Position&#39;) plt.colorbar() plt.show() . Multi Headed Scaled Dot Product Attention . . $ Large Attention(Q,K,V) = softmax left( frac{QK^{T}}{ sqrt{d_k}} right)V $ . class mha(nn.Module): # multi headed attention def __init__(self, d_model=512, nheads=8, dp_rate=0.1): super().__init__() self.nheads = nheads self.qw = nn.Linear(d_model, d_model) self.kw = nn.Linear(d_model, d_model) self.vw = nn.Linear(d_model, d_model) self.out_proj = nn.Linear(d_model, d_model) self.depth = d_model//nheads self.dropout = nn.Dropout(dp_rate) def scaled_dot_product_attention(self, q, k, v, attn_mask=None): # q,k[B, nheads, seq_len, depth] scale = q.size(-1)**-0.5 q = q*scale attn = torch.matmul(q, k.transpose(-2,-1)) # [B, nheads, seq_len_q, seq_len_k] if attn_mask is not None: attn += attn_mask attn = attn.softmax(dim=-1) attn = self.dropout(attn) out = torch.matmul(attn, v) # [B, nheads, seq_len_q, depth_v] return out, attn def forward(self, query, key, value, attn_mask=None): # [B, seq_len, d_model] batch_size, seq_len, d_model = query.shape q,k,v = self.qw(query), self.kw(key), self.vw(value) # [B, seq_len, d_model] q = q.view(batch_size, -1, self.nheads, self.depth) # [B, seq_len_q, nheads, depth] # d_model = nheads * depth k = k.view(batch_size, -1, self.nheads, self.depth) # [B, seq_len_k, nheads, depth] v = v.view(batch_size, -1, self.nheads, self.depth) # [B, seq_len_v, nheads, depth] q = q.transpose(-2,-3) # [B, nheads, seq_len_q, depth] k = k.transpose(-2,-3) # [B, nheads, seq_len_k, depth] v = v.transpose(-2,-3) # [B, nheads, seq_len_v, depth] out, attn = self.scaled_dot_product_attention(q,k,v,attn_mask) # [B, nheads, seq_len_q, depth] out = out.transpose(-2,-3) # [B, seq_len_q, nheads, depth] out = out.reshape(batch_size, -1, d_model) # [B, seq_len_q, d_model] out = self.out_proj(out) # [B, seq_len_q, d_model] return out, attn . Transformer . . def get_ffn(d_model, dp_rate=0.1, activation=&#39;relu&#39;): if activation == &#39;relu&#39;: act_lyr = nn.ReLU() elif activation == &#39;gelu&#39;: act_lyr = nn.GELU() return nn.Sequential(OrderedDict([ (&#39;lin1&#39;, nn.Linear(d_model, d_model*4)), (&#39;activation&#39;, act_lyr), (&#39;dropout&#39;, nn.Dropout(dp_rate)), (&#39;lin2&#39;, nn.Linear(d_model*4, d_model)) ])) . class TransformerEncoderLayer(nn.Module): def __init__(self, d_model=512, nheads=8, dp_rate=0.1, activation=&#39;relu&#39;): super().__init__() self.attn = mha(d_model=d_model, nheads=nheads, dp_rate=dp_rate) self.dropout1 = nn.Dropout(dp_rate) self.ln1 = nn.LayerNorm(d_model) self.ffn = get_ffn(d_model, dp_rate, activation) self.dropout2 = nn.Dropout(dp_rate) self.ln2 = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): # [B, seq_len, d_model] x = self.ln1(x + self.dropout1(self.attn(x,x,x, attn_mask)[0])) x = self.ln2(x + self.dropout2(self.ffn(x))) return x . class TransformerDecoderLayer(nn.Module): def __init__(self, d_model=512, nheads=8, dp_rate=0.1, activation=&#39;relu&#39;): super().__init__() self.attn1 = mha(d_model=d_model, nheads=nheads, dp_rate=dp_rate) self.dropout1 = nn.Dropout(dp_rate) self.ln1 = nn.LayerNorm(d_model) self.attn2 = mha(d_model=d_model, nheads=nheads, dp_rate=dp_rate) self.dropout2 = nn.Dropout(dp_rate) self.ln2 = nn.LayerNorm(d_model) self.ffn = get_ffn(d_model, dp_rate, activation) self.dropout3 = nn.Dropout(dp_rate) self.ln3 = nn.LayerNorm(d_model) def forward(self, x, enc_output, src_mask=None, tgt_mask=None): # [B, seq_len, d_model] x = self.ln1(x + self.dropout1(self.attn1(x,x,x, tgt_mask)[0])) x = self.ln2(x + self.dropout2(self.attn2(x,enc_output,enc_output, src_mask)[0])) x = self.ln3(x + self.dropout3(self.ffn(x))) return x . class TransformerEncoder(nn.Module): def __init__(self, src_vocab_size, nlayers=6, d_model=512, nheads=8, dp_rate=0.1, activation=&#39;relu&#39;, use_pe=True): super().__init__() self.d_model = d_model self.embedding = nn.Embedding(src_vocab_size, d_model) self.use_pe = use_pe if self.use_pe: pos_enc = torch.tensor(positional_encoding(max_seq_len, d_model), requires_grad=False) self.register_buffer(&#39;pos_enc&#39;, pos_enc) self.dropout = nn.Dropout(dp_rate) self.enc_layers = nn.ModuleList([TransformerEncoderLayer(d_model, nheads, dp_rate, activation) for _ in range(nlayers)]) # repeat encoder layer N times def forward(self, x, src_mask=None): # [B, seq_len] seq_len = x.size(1) x = self.embedding(x) # [B, seq_len, d_model] x = x * (self.d_model**0.5) if self.use_pe: x = x + self.pos_enc[:,:seq_len] x = self.dropout(x) for lyr in self.enc_layers: x = lyr(x, src_mask) return x . class TransformerDecoder(nn.Module): def __init__(self, tgt_vocab_size, nlayers=6, d_model=512, nheads=8, dp_rate=0.1, activation=&#39;relu&#39;, use_pe=True): super().__init__() self.d_model = d_model self.embedding = nn.Embedding(tgt_vocab_size, d_model) self.use_pe = use_pe if self.use_pe: pos_enc = torch.tensor(positional_encoding(max_seq_len, d_model), requires_grad=False) self.register_buffer(&#39;pos_enc&#39;, pos_enc) self.dropout = nn.Dropout(dp_rate) self.dec_layers = nn.ModuleList([TransformerDecoderLayer(d_model, nheads, dp_rate, activation) for _ in range(nlayers)]) # repeat decoder layer N times def forward(self, x, enc_output, src_mask=None, tgt_mask=None): # [B, seq_len] seq_len = x.size(1) x = self.embedding(x) # [B, seq_len, d_model] x = x * (self.d_model**0.5) if self.use_pe: x = x + self.pos_enc[:,:seq_len] x = self.dropout(x) for lyr in self.dec_layers: x = lyr(x, enc_output, src_mask, tgt_mask) return x . class Transformer(nn.Module): def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nheads=8, nlayers=6, dp_rate=0.1, activation=&#39;relu&#39;, use_pe=True): super().__init__() self.encoder = TransformerEncoder(src_vocab_size, nlayers, d_model, nheads, dp_rate, activation, use_pe) self.decoder = TransformerDecoder(tgt_vocab_size, nlayers, d_model, nheads, dp_rate, activation, use_pe) self.fout = nn.Linear(d_model, tgt_vocab_size) self.reset_parameters() def reset_parameters(self): for p in self.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) def forward(self, src, tgt, src_mask=None, tgt_mask=None): # [B, seq_len] x = self.encoder(src, src_mask) x = self.decoder(tgt, x, src_mask, tgt_mask) x = self.fout(x) return x . Dataset . import torchtext from torchtext.utils import download_from_url, extract_archive from torchtext.data.utils import get_tokenizer from torchtext.vocab import Vocab from collections import Counter from itertools import chain import io . url_base = &#39;https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/&#39; train_urls = (&#39;train.de.gz&#39;, &#39;train.en.gz&#39;) val_urls = (&#39;val.de.gz&#39;, &#39;val.en.gz&#39;) test_urls = (&#39;test_2016_flickr.de.gz&#39;, &#39;test_2016_flickr.en.gz&#39;) . train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls] val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls] test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls] . de_tokenizer = get_tokenizer(&#39;spacy&#39;, language=&#39;de_core_news_sm&#39;) en_tokenizer = get_tokenizer(&#39;spacy&#39;, language=&#39;en_core_web_sm&#39;) . def build_vocab(filepath, tokenizer): with io.open(filepath, encoding=&quot;utf8&quot;) as f: lines = &#39;&#39;.join(f.readlines()).replace(&#39; n&#39;, &#39; &#39;) counter = Counter(tokenizer(lines)) return Vocab(counter, specials=[&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;bos&gt;&#39;, &#39;&lt;eos&gt;&#39;]) . de_vocab = build_vocab(train_filepaths[0], de_tokenizer) en_vocab = build_vocab(train_filepaths[1], en_tokenizer) . PAD_IDX = de_vocab[&#39;&lt;pad&gt;&#39;] BOS_IDX = de_vocab[&#39;&lt;bos&gt;&#39;] EOS_IDX = de_vocab[&#39;&lt;eos&gt;&#39;] . def split_toks(toks, sep=&#39; n&#39;): # tokenizing line by line was too slow on colab. chunk = [] for v in toks: if sep in v: yield chunk chunk = [] else: chunk.append(v) . def data_preprocess(filepaths): with io.open(filepaths[0], encoding=&quot;utf8&quot;) as f: de_full = &#39; &#39;.join(f.readlines()) with io.open(filepaths[1], encoding=&quot;utf8&quot;) as f: en_full = &#39; &#39;.join(f.readlines()) de_toks = de_tokenizer(de_full) en_toks = en_tokenizer(en_full) de_iter = split_toks(de_toks) en_iter = split_toks(en_toks) data = [] for de_line, en_line in zip(de_iter, en_iter): de_tensor = torch.tensor([BOS_IDX] + [de_vocab[token] for token in de_line] + [EOS_IDX]) en_tensor = torch.tensor([BOS_IDX] + [en_vocab[token] for token in en_line] + [EOS_IDX]) data.append((de_tensor, en_tensor)) return data . train_data = data_preprocess(train_filepaths) val_data = data_preprocess(val_filepaths) test_data = data_preprocess(test_filepaths) . from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader . def generate_batch(data_batch): de_batch, en_batch = [], [] for (de_item, en_item) in data_batch: de_batch.append(de_item) en_batch.append(en_item) de_batch = pad_sequence(de_batch, batch_first=True, padding_value=PAD_IDX) en_batch = pad_sequence(en_batch, batch_first=True, padding_value=PAD_IDX) return de_batch, en_batch . BATCH_SIZE = 256 . train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, num_workers=2) valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, num_workers=2) test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, num_workers=2) . Masking . def subsequent_mask(sz): mask = torch.ones((1,1,sz,sz), device=device, dtype=bool).tril() mask.logical_not_() return mask def mask_fill_inf(mask): return mask.float().masked_fill(mask, float(&#39;-inf&#39;)) def create_mask(src, tgt): # [B, seq_len] tgt_mask = subsequent_mask(tgt.size(1)) src_padding_mask = (src == PAD_IDX)[:,None,None,:] # [B, nheads, seq_len_q, seq_len_k] tgt_padding_mask = (tgt == PAD_IDX)[:,None,None,:] # [B, nheads, seq_len_q, seq_len_k] tgt_mask = tgt_mask.logical_or(tgt_padding_mask) # combine mask for look ahead and tgt padding. return mask_fill_inf(src_padding_mask), mask_fill_inf(tgt_mask) . Model Parameters . import wandb . config_defaults = { &#39;src_vocab_size&#39; : len(de_vocab), &#39;tgt_vocab_size&#39; : len(en_vocab), &#39;BATCH_SIZE&#39; : BATCH_SIZE, &#39;d_model&#39; : 512, &#39;nheads&#39; : 16, &#39;nlayers&#39; : 4, &#39;dp_rate&#39; : 0.1, &#39;activation&#39; : &#39;gelu&#39;, &#39;ilr&#39; : 1, &#39;betas&#39; : (0.9, 0.98), &#39;eps&#39; : 1e-9, &#39;use_amp&#39; : True, &#39;use_pe&#39; : True, &#39;log_interval&#39; : 5, } . # CONFIG = run.config . run = wandb.init(project=&quot;Basic_Transformer&quot;, entity=&quot;shivamshrirao&quot;, config=config_defaults) CONFIG = wandb.config . Tracking run with wandb version 0.10.32 Syncing run astral-pine-8 to Weights &amp; Biases (Documentation). Project page: https://wandb.ai/shivamshrirao/Basic_Transformer Run page: https://wandb.ai/shivamshrirao/Basic_Transformer/runs/11981s09 Run data is saved locally in /content/wandb/run-20210614_225819-11981s09 model = Transformer(src_vocab_size = CONFIG[&#39;src_vocab_size&#39;], tgt_vocab_size = CONFIG[&#39;tgt_vocab_size&#39;], d_model = CONFIG[&#39;d_model&#39;], nheads = CONFIG[&#39;nheads&#39;], nlayers = CONFIG[&#39;nlayers&#39;], dp_rate = CONFIG[&#39;dp_rate&#39;], activation = CONFIG[&#39;activation&#39;], use_pe = CONFIG[&#39;use_pe&#39;]).to(device) . def lr_schedule(step, d_model=1024*8, warmup_steps=400): # return 1 step = max(1,step) arg1 = step ** -0.5 arg2 = step * (warmup_steps ** -1.5) return (d_model ** -0.5) * min(arg1, arg2) . plt.plot(list(map(lr_schedule, range(114*40)))) plt.show() . loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX) optimizer = torch.optim.Adam( model.parameters(), lr=CONFIG[&#39;ilr&#39;], betas=CONFIG[&#39;betas&#39;], eps=CONFIG[&#39;eps&#39;] ) scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule) scaler = torch.cuda.amp.GradScaler(enabled=CONFIG[&#39;use_amp&#39;]) . from torch.utils.tensorboard import SummaryWriter . writer = SummaryWriter(run.dir+&#39;/tensorboard_logs/custom_transformer&#39;) . src, tgt = next(iter(train_iter)) src = src.to(device) tgt = tgt.to(device) writer.add_graph(model, (src, tgt)) writer.close() del src, tgt . wandb.watch(model, log=None) . [&lt;wandb.wandb_torch.TorchGraph at 0x7f07a50a3110&gt;] . def train_epoch(model, train_iter, optimizer, scaler, scheduler, epoch=1, use_amp=True, log_interval=10): model.train() losses = 0 with tqdm(enumerate(train_iter), total=len(train_iter), desc=f&quot;Epoch {epoch}&quot;) as pbar: for idx, (src, tgt) in pbar: src = src.to(device) tgt = tgt.to(device) tgt_inp = tgt[:,:-1] # give input until before the last word. tgt_out = tgt[:,1:] # predict the last word based on input and already predicted sentence. (auto-regressive) src_mask, tgt_mask = create_mask(src, tgt_inp) optimizer.zero_grad(set_to_none=True) with torch.cuda.amp.autocast(enabled=use_amp): logits = model(src, tgt_inp, src_mask, tgt_mask) loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1)) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() scheduler.step() losses+= loss.item() avg_loss = losses/(idx+1) curr_lr = optimizer.param_groups[0][&#39;lr&#39;] if not idx%log_interval: wandb.log({&#39;loss&#39;: avg_loss, &#39;lr&#39;: curr_lr}) pbar.set_postfix({&#39;loss&#39;: f&#39;{avg_loss:.3f}&#39;, &#39;lr&#39;: curr_lr}) return losses/len(train_iter) . def evaluate(model, val_iter): model.eval() losses = 0 with tqdm(enumerate(val_iter), total=len(val_iter), desc=&quot;Evaluating&quot;) as pbar: for idx, (src, tgt) in pbar: src = src.to(device) tgt = tgt.to(device) tgt_inp = tgt[:,:-1] # give input until before the last word. tgt_out = tgt[:,1:] # predict the last word based on input and already predicted sentence. (auto-regressive) src_mask, tgt_mask = create_mask(src, tgt_inp) logits = model(src, tgt_inp, src_mask, tgt_mask) loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1)) losses+= loss.item() pbar.set_postfix({&#39;val_loss&#39;: f&quot;{losses/(idx+1):.3f}&quot;}) return losses/len(val_iter) . init_epoch = 1 . def save_model(model, optimizer, epoch): torch.save({ &#39;model_state_dict&#39;: model.state_dict(), &#39;optimizer_state_dict&#39;: optimizer.state_dict(), &#39;epoch&#39;: epoch, }, run.dir + &#39;/model.pth&#39;) . . . # model.load_state_dict(checkpoint[&#39;model_state_dict&#39;]) # optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;]) # init_epoch = checkpoint[&#39;epoch&#39;] . NUM_EPOCHS = 20 . for epoch in range(init_epoch, NUM_EPOCHS+1): train_loss = train_epoch(model, train_iter, optimizer, scaler, scheduler, epoch, CONFIG[&#39;use_amp&#39;], CONFIG[&#39;log_interval&#39;]) val_loss = evaluate(model, valid_iter) wandb.log({&quot;val_loss&quot;: val_loss, &quot;epoch&quot;: epoch}) print(f&quot;Epoch: {epoch}/{NUM_EPOCHS}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f} n&quot;) # if not epoch%10: # save_model(model, optimizer, epoch) . Epoch: 1/20, Train loss: 6.648, Val loss: 4.863 Epoch: 2/20, Train loss: 4.237, Val loss: 3.710 Epoch: 3/20, Train loss: 3.467, Val loss: 3.264 Epoch: 4/20, Train loss: 3.065, Val loss: 3.003 Epoch: 5/20, Train loss: 2.785, Val loss: 2.862 Epoch: 6/20, Train loss: 2.569, Val loss: 2.742 Epoch: 7/20, Train loss: 2.392, Val loss: 2.657 Epoch: 8/20, Train loss: 2.241, Val loss: 2.606 Epoch: 9/20, Train loss: 2.112, Val loss: 2.557 Epoch: 10/20, Train loss: 1.992, Val loss: 2.539 Epoch: 11/20, Train loss: 1.885, Val loss: 2.505 Epoch: 12/20, Train loss: 1.789, Val loss: 2.484 Epoch: 13/20, Train loss: 1.695, Val loss: 2.455 Epoch: 14/20, Train loss: 1.611, Val loss: 2.463 Epoch: 15/20, Train loss: 1.536, Val loss: 2.433 Epoch: 16/20, Train loss: 1.460, Val loss: 2.434 Epoch: 17/20, Train loss: 1.391, Val loss: 2.427 Epoch: 18/20, Train loss: 1.324, Val loss: 2.442 Epoch: 19/20, Train loss: 1.265, Val loss: 2.435 Epoch: 20/20, Train loss: 1.202, Val loss: 2.442 . init_epoch = epoch . Make Predictions . def greedy_decode(model, src, src_mask, max_len, start_symbol): model.eval() src = src.to(device) src_mask = src_mask.to(device) enc_output = model.encoder(src, src_mask) tgt = torch.LongTensor([start_symbol]).reshape(1,-1).to(device) for i in range(max_len): tgt_mask = mask_fill_inf(subsequent_mask(tgt.size(1))) out = model.decoder(tgt, enc_output, src_mask, tgt_mask) prob = model.fout(out[:,-1]) _, next_word = torch.max(prob, dim = 1) next_word = next_word.item() tgt = torch.cat([tgt, torch.LongTensor([next_word]).reshape(1,-1).to(device)], dim=1) if next_word == EOS_IDX: break return tgt def translate(model, src, src_vocab, tgt_vocab, src_tokenizer): tokens = [BOS_IDX] + [src_vocab[tok] for tok in src_tokenizer(src)]+ [EOS_IDX] src = torch.LongTensor(tokens).reshape(1,-1) src_mask = torch.zeros_like(src).float()[:,None,None,:] tgt = greedy_decode(model, src, src_mask, max_len=len(tokens)+10, start_symbol=BOS_IDX).flatten() return &quot; &quot;.join([tgt_vocab.itos[tok] for tok in tgt]).replace(&quot;&lt;bos&gt;&quot;, &quot;&quot;).replace(&quot;&lt;eos&gt;&quot;, &quot;&quot;) . translate(model, &quot;Eine Gruppe von Menschen steht vor einem Iglu .&quot;, de_vocab, en_vocab, de_tokenizer) . &#39; A group of people in front of an igloo &#39; . run.finish() . Waiting for W&amp;B process to finish, PID 2945Program ended successfully. Find user logs for this run at: /content/wandb/run-20210614_225819-11981s09/logs/debug.log Find internal logs for this run at: /content/wandb/run-20210614_225819-11981s09/logs/debug-internal.log Run summary: . loss | 1.20159 | . lr | 0.00016 | . _runtime | 872 | . _timestamp | 1623712371 | . _step | 479 | . val_loss | 2.44236 | . epoch | 20 | . Run history: . loss | █▇▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ | . lr | ▁▃▅██▇▇▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃ | . _runtime | ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███ | . _timestamp | ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███ | . _step | ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███ | . val_loss | █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁ | . epoch | ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██ | . Synced 5 W&amp;B file(s), 1 media file(s), 23 artifact file(s) and 2 other file(s) Synced astral-pine-8: https://wandb.ai/shivamshrirao/Basic_Transformer/runs/11981s09",
            "url": "https://shivamshrirao.github.io/blog/2021/06/19/Transformer-in-Pytorch.html",
            "relUrl": "/2021/06/19/Transformer-in-Pytorch.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://shivamshrirao.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://shivamshrirao.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://shivamshrirao.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shivamshrirao.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}