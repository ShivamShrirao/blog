<h1 id="stats-basics">Stats Basics</h1>
<blockquote>
  <p>Just some basic stats concepts for quick revision.</p>
</blockquote>

<ul>
  <li>toc: true</li>
  <li>comments: true</li>
  <li>hide: false</li>
  <li>image: images/stats/high_bias.png</li>
</ul>

<h1 id="cross-validation">Cross Validation</h1>

<ul>
  <li>Split the training data into multiple folds, eg. Four folds of 25% each.</li>
  <li>For each fold as testing, and rest as training train a model each time.</li>
  <li>Do same with different models. Choose the one which gets the most folds correct.</li>
</ul>

<h1 id="sensitivity--specificity">Sensitivity &amp; Specificity</h1>

<ul>
  <li><strong>Sensitivity</strong>: Percentage of positives correctly identified. (True Positive rate)
    <ul>
      <li>Eg.: Percentage of patients correctly identified to have a disease among all actually having disease.</li>
      <li>Sensitivity = $\frac{TP}{TP+FN}$</li>
    </ul>
  </li>
  <li><strong>Specificity</strong>: Percentage of negatives correctly identified. (True Negative rate)
    <ul>
      <li>Eg.: Percentage of patients correctly identified to NOT have a disease among all actually NOT having disease.</li>
      <li>Specificity = $\frac{TN}{TN+FP}$</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Real Has</th>
      <th>Real Not Has</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Predicted Has</td>
      <td>TP</td>
      <td>FP</td>
    </tr>
    <tr>
      <td>Predicted Not Has</td>
      <td>FN</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<h1 id="precision--recall">Precision &amp; Recall</h1>

<ul>
  <li><strong>Precision</strong>: Proportion of positives correctly identified.
    <ul>
      <li>Eg.: Number of patients actually having the disease among all prediicted to be having the disease.</li>
      <li>Precision = $\frac{TP}{TP+FP}$</li>
    </ul>
  </li>
  <li><strong>Recall</strong> = <a href="#Sensitivity-&amp;-Specificity">Sensitivity</a>.</li>
</ul>

<h1 id="bias--variance">Bias &amp; Variance</h1>

<ul>
  <li><strong>Bias</strong>: “Prior assumptions” in model/algorithm preventing it to fit on data.
    <ul>
      <li>Eg: Linear regression will be a straight line so can’t fit on curved data, therefore it has high bias and low variance.
  <img src="images/stats/high_bias.png" alt="" title="https://youtu.be/EuBBz3bI-aA" /><br />
  <em>Image Credit: <a href="https://youtu.be/EuBBz3bI-aA">Machine Learning Fundamentals: Bias and Variance</a></em></li>
    </ul>
  </li>
  <li><strong>Variance</strong>: Model/algorithm is can “vary” itself alot and thus can overfit on data.
    <ul>
      <li>Eg: Lot of curves going exactly through all data points, model won’t generalize, therefore it has high variance and low bias.
  <img src="images/stats/high_variance.png" alt="" title="https://youtu.be/EuBBz3bI-aA" /><br />
  <em>Image Credit: <a href="https://youtu.be/EuBBz3bI-aA">Machine Learning Fundamentals: Bias and Variance</a></em></li>
    </ul>
  </li>
  <li><strong>Bias Variance Tradeoff</strong>: We gotta find a good comprise among the two to find best model.</li>
</ul>

<p><strong>Intersting Fact</strong>: Check out double descent in deep learning, highly over-parameterized models can at first overfit, perform worse but then start to perform better again.<br />
An intuitive possible explanation: https://twitter.com/daniela_witten/status/1292293102103748609</p>

<h1 id="roc--auc">ROC &amp; AUC</h1>

<ul>
  <li><strong>Receiver Operator Characterstic (ROC)</strong>: Plot True Positive Rate vs False Positive Rate of model for different classification thresholds.
    <ul>
      <li>Decide what works better for you, if you want classify positives more correctly and some false positives,</li>
      <li>OR lesser true positives for no false positives.<br />
  <img src="images/stats/roc.png" alt="" title="https://youtu.be/4jRBRDbJemM" /><br />
  <em>Image Credit: <a href="https://youtu.be/4jRBRDbJemM">ROC and AUC, Clearly Explained!</a></em></li>
    </ul>
  </li>
  <li><strong>Area Under Curve (AUC)</strong>: Compare ROC graphs.
    <ul>
      <li>Graph with higher area better.</li>
      <li>False positive rate can often be replaced with Precision.<br />
  <img src="images/stats/auc.png" alt="" title="https://youtu.be/4jRBRDbJemM" /><br />
  <em>Image Credit: <a href="https://youtu.be/4jRBRDbJemM">ROC and AUC, Clearly Explained!</a></em></li>
    </ul>
  </li>
</ul>

<h1 id="residuals">Residuals</h1>
<ul>
  <li><strong>Residuals</strong> : Vertical distance of data point from line.</li>
  <li><strong>R squared</strong> : $\large1 -\frac{\text{Sum of squares of Residuals}}{\text{Total sum of squares(variance)}}$.<br />
0 -&gt; Worst, 1 -&gt; Best.</li>
</ul>

<p>$log(odds) = log(\frac{p}{1-p})$</p>

<h1 id="logistic-regression">Logistic Regression</h1>

<p>Probability between 0 and 1.
Convert to log(odds) on y axis to compare to linear regression.</p>

<ul>
  <li><strong>Uses Maximum Likelihood</strong>: Log adds for datapoints converted to probability.</li>
  <li><strong>Log Likelihood</strong>: Sum of logs of probabilities or multiplication of probabilities.<br />
<img src="images/stats/logistic_likelihood.png" alt="" title="https://youtu.be/BfKanl1aSG0" /><br />
  <em>Image Credit: <a href="https://youtu.be/BfKanl1aSG0">Logistic Regression Details Pt 2: Maximum Likelihood</a></em></li>
</ul>

<h1 id="ridge-l2-regularization">Ridge (L2) Regularization</h1>

<ul>
  <li>Test</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
